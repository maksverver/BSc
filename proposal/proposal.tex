\documentclass{acm_proc_article-sp}

\usepackage{pslatex}
\usepackage{epsfig}
\usepackage{appendix}
\usepackage{float}

\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Program}

\begin{document}

\title{Evaluation of Cache-Oblivious Data Structures}
\subtitle{Research Proposal (draft)}

\numberofauthors{1}
\author{Maks Verver\\ \email{m.verver@student.utwente.nl}}

% Obligatory permission block
\toappear{
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission.

\textit{\small 8$^{th}$ Twente Student Conference on IT, Enschede, June, 2008}

Copyright 2008, University of Twente, Faculty of Electrical Engineering,
Mathematics and Computer Science}
% End of obligatory permission block

\maketitle

% TODO: abstract
%\begin{abstract}
%The cache-oblivious memory model is a model that allows algorithms and data structures to be evaluated in terms of memory transfers in a two-level memory hierarchy. Efficient data structurs and algorithms that operate on them are of interest because they are used in more complex, domain-specific algorithms.  This proposal suggests that emperical evidence is necessary to determine wether cache-oblivious data structures that have been described previously, perform well enough in practice so that they may replace traditional data structures.
%\end{abstract}

\keywords{cache efficiency, locality of reference, algorithms}

\section{Introduction}
%[A fundamental part of theoretical computer science is the study of algorithms: formal descriptions of how computations may be performed. In order for algorithms to be able to process information efficiently, it must be stored in a systematic way, called a data structure. Algorithms and data structures therefore complement each other: while algorithms describe how a computer may process information, data structures describe how information is stored in a (model of a) computer's memory.]

A fundamental part of theoretical computer science is the study of algorithms (formal descriptions of how computations may be performed) and data structures (descriptions of how information is organized and stored in computers). Traditionally, algorithms have been evaluated in a simplified model of computation. In this model it is assumed that a computer executes an algorithm in discrete steps. At each step it performs one elementary operation (e.g. comparing two numbers, adding one to another, storing a value in memory, et cetera). Each elementary operation is performed within a constant time. In this model, both storing and retrieving data values in memory is considered to be an elementary operation.

This model is close enough to the way computers work to be extremely useful in the development and analysis of data structures and algorithms that work well in practice. However, like every model, it is a simplification of reality. One of the simplifications is the assumption that storing and retrieving data in memory is a constant-time operation (i.e. the uniform memory model). Advancements in software and hardware design over the last two decades have caused this assumption to be increasingly detached from reality.

One of the reasons this assumption fails is that modern operating systems employ virtual memory management, which allows (slower) disk-based storage to be dynamically substituted for (faster) main memory when the latter is in short supply. Another is that as processor speeds have increased greatly, the time required to transfer data between processor and main memory has become a bottleneck for many types of computations. Hardware architects have added faster (but small) cache memory at various points in the computer architecture to reduce this problem, but this increases the differences in access time between different types of memory.

It should be clear by now that a modern computer system lacks a single, central memory storage. Instead, there is a hierarchy of memory storage types, including at least the processor registers, L1/L2 cache, main memory and disk storage. At each of these levels, the next storage type is both significantly faster and significantly larger than the previous type. Typically, the processor can directly manipulate data contained in its registers only. To access data in a lower level in the memory hierarchy, the data must be transfered upward through the memory hierarchy. Memory is transfered in blocks of data of a fixed size (although bulk transfers involving multiple blocks of data at once are also supported at the memory and disk level).

% TODO: add picture of memory hierarchy

Apparently, the memory model used in real computer systems is quite a bit more complex than the uniform memory model assumed. Given this reality, the performance of many existing algorithms and data structures can be improved by taking the existence of a memory hierarchy into account. To be able to analyze these algorithms and data structures formally, a different, more realistic model of the cost of accessing data in memory is required.

\subsection{Alternative Memory Models}
In the external memory model, as described by Aggarwal and Vitter \cite{aggarwal1988ioc}, a distinction is made between internal memory (which is limited in size) and external memory (which is virtually unlimited). The external memory is subdivided into blocks of a fixed size and only entire blocks of data can be transferred between the two memories. Although Aggarwal and Vitter focus on magnetic disk as a storage medium for external memory, the model can be generalized to apply to every pair of adjacent levels in the memory hierarchy. In that case, the possibility of transferring several data blocks at once (bulk transfer) may have to be dropped.

The external memory model has the limitation that it only describes two levels of storage, while we have seen that in practice there is a larger memory hierarchy consisting of multiple levels. Even though the external memory model can be used to describe any pair of adjacent levels, a particular algorithm can only be tuned to one. Aggarwal, Alpern, Chandra and Snir \cite{aggarwal1987mhm} introduced a hierarchical memory model in which the cost of accessing a value at specific memory address is described by a non-decreasing function of this address, which means that accessing data at higher addresses can be slower than at lower addresses. This more closely resembles a real memory hierarchy.

\subsection{Cache-Oblivious Memory Model}
The external memory model was generalized and simplified into the cache-oblivious model \cite{prokop1999coa} which consists of an infinitely large external memory and an internal memory of size M which operates as a cache for the external memory. Data is transferred between the two in aligned data blocks of size B. In this model (and unlike the external memory model) the values of parameters like M and B are not known to algorithms and data structures, so they cannot be referenced explicitly in their definition. Of course, analysis in terms of memory transfers does involve these parameters, so the number of memory transfers performed is still a function of M and B (and other parameters relevant to the problem). Like the external memory model, the cache-oblivious model can be used to describe every pair of adjacent levels in the memory hierarchy.

It should be noted that the cache-oblivious model can also be considered similar to the hierarchical model discussed above. The main difference between the two is that in the hierarchical model the application has explicit control over which data are placed at the fastest locations, since these have the lowest addresses, while in the cache-oblivious model the internal memory acts as a transparent cache of the external memory, and the application has no explicit control over which values are kept in the cache.
% This makes it easier to design algorithms in the oblivious model, because in the HMM the algorithm has the responsibility to use the cache correctly.

The key advantage of the cache-oblivious model is that algorithms that perform optimally when evaluated in this model are implicitly tuned to all levels in the real memory hierarchy at once. Therefore, these algorithms might perform better than algorithms that are tuned to a specific level in the hierarchy only.

Erik D. Demaine gives an introduction into the cache-oblivious memory model and an overview of a selection of cache-oblivious data structures and algorithms \cite{demaine2002coa}. He also motivates the simplifications made in the cache-oblivious memory model, such as the assumption of full cache associativity and an optimal replacement policy.

\subsection{Categories}
The development of the different memory models means data structures and algorithms can be classified depending on the assumptions about their environment used in their definitions.
\begin{list}{}{}
\item \textbf{Cache-aware} data structures and algorithms perform optimally in the external memory model, but require parametrization with some or all properties of the cache layout (such as block size or cache memory size).
\item \textbf{Cache-unaware} data structures and algorithms are designed with the uniform memory model in mind and may therefore not perform optimally in the external memory model.
\item \textbf{Cache-oblivious} data structures and algorithms are designed to perform optimally in the cache-oblivious model, meaning they are also optimal in the external memory model (in the asymptotic sense and up to a constant factor of memory transfers) without requiring tuning of parameters.
\end{list}

\section{Related Work}
Many data structures and algorithms have been analyzed in the cache-oblivious model. In many cases, new data structures and algorithms have been introduced that perform optimally in this model. Prokop presents asymptotically optimal cache-oblivious algorithms for matrix transposition, fast Fourier transformation and sorting \cite{prokop1999coa}.

Bender, Demaine and Farach-Colton designed a cache-oblivious data structure that supports the same operations as a B-tree \cite{bender2005cob} achieving optimal complexity bounds on search and nearly-optimal bounds on insertion. Later, Bender, Duan, Iacono and Wu simplified the data structure \cite{bender2004lpc} while preserving the supported operations and complexity bounds and adding finger searches. The latter data structure consists of a sparse array (stored in a linear amount of memory) with a static search tree in van Emde Boas layout used to find elements.

Vitter presents a theoretical survey of algorithms evaluated in a parallel disk model \cite{vitter2001ema}, which is a refinement of the external memory model described by Aggarwal and Vitter, but only allows parallel transfer of multiple blocks from different disks, which is more realistic. Unfortunately, his survey lacks empirical results.

% [TODO: add more recent related work]

% [TODO: find empirical results if they exists, or note lack of empirical data]

\section{Problem Statement}
As described above, several cache-oblivious data structures and algorithms have been proposed. Complexity analysis shows that the proposed solutions are asymptotically optimal. However, in software engineering practice we are not only interested in computational complexity, but also in the practical performance of data structures and algorithms. Indeed, many algorithms that have suboptimal computational complexity are actually widely used because they perform well in practice (for example, sorting algorithms like Quicksort and Shell sort).
%The converse is true as well: some algorithms that are better than widely used alternatives in terms of computational complexity have bad performance on practical data sets, use too much memory, or are too hard to implement correctly (for example, linear time construction of suffix arrays).

This raises the question whether cache-oblivious data structures are actually preferable to traditional data structures in practice. The theory suggests that cache-oblivious algorithms should perform well, but this can only be confirmed by evaluating their performance in a realistic environment and operating on a realistic data set. However, such empirical data is scarce when it comes to cache-oblivious data structures, as existing research has focused mainly on theoretical analysis. The goal of the proposed research project is to perform an empirical evaluation of previously described cache-oblivious data structures and compare them with their traditional counterparts (both cache-aware and cache-unaware data structures).

From the application programmer's point of view, data structures and algorithms can be grouped together based on the functionality they provide. For example, all comparison-based sorting algorithms can be considered to provide equivalent functionality, even if their performance is different. Since it is not sensible to compare data structures which provide different functionality, it is necessary to limit the scope of the project to data structures that provide similar functions. As will described below, we will limit ourselves to one particular kind of data structures, in the hope that our results may give some insight in the performance of cache-oblivious data structures in general.

\subsection{Research Questions}
Following from the above, the main question to be answered is:
\begin{list}{}{}
\item Do cache-oblivious data structures have practical merit?
\end{list}

More specifically, these questions will be answered:
\newcounter{cnt}
\begin{list}{\arabic{cnt}.}{\usecounter{cnt}}
\item How do cache-oblivious data structures perform when compared to cache-aware data structures?
\item How do cache-oblivious data structures perform when compared to cache-unaware data structures?
\item How does the memory use of cache-oblivious data structures relate to the memory used by other data structures?
\end{list}
Because we are primarily interested in practical performance, the measure of performance will be the actual run-time of the experiments. Memory use is of interest as well (if only as a secondary criterion) as it may reveal that there is a trade-off between memory use and execution time of data structures.

\section{Research Approach}

% To add:
% - language of implementation
% - platform used
% - data used
% - how to test

The goal of the project is to evaluate a variety of data structures in a practical setting. For this, we need a realistic scenario that makes heavy use of data structures of which cache-oblivious implementations are available. A scenario that suits these needs, is that of a state space search algorithm (or, in short: state search). The purpose of such an algorithm is to find an optimal path from a (set of) given initial state(s) to a goal state. State search algorithms are often used to solve reachability problems, which occur for example in formal model checking. The main data structures used are a queue (either a regular queue or, if search heuristics are used, a priority queue) and a dynamic set. An example of a simple state search algorithm is given in program \ref{prog-search}.

\begin{program}
\begin{verbatim}
Queue queue = new Queue;
Set visited = new Set;
queue.insert(initial_state);
while (!queue.empty()) {
    State state = queue.extract();
    for (State s : successors(state)) {
        if (isGoal(s)) break;
        if (visited.contains(s) == false) {
            visited.insert(s);
            queue.insert(s);
        }
    }
}
\end{verbatim}
\caption{Pseudo-code for a simple state search algorithm.}
\label{prog-search}
\end{program}

In this program, a queue is a data structure where elements can be inserted and extracted on a first-in-first-out basis. A dynamic set is a data structure that models a set to which elements may be added (and optionally, from which they may be removed) and which may be queried to see if the set contains a particular element.

Since implementing a normal queue is fairly straightforward, the execution time of the algorithm will be dominated by the performance of the set, assuming domain-specific functions (like \verb#successors# and \verb#isGoal# in the sample code) are efficient. The main focus of the research project will therefore be on alternative implementations of the set data structure.

%The situation changes, of course, when instead of a first-in-first-out queue a priority queue is used. This is often the case when we are searching for a shortest path (instead of any path that reaches the goal) or when heuristics are used in the search. In this case, the complexity of the queue implementation increases. If time allows it, the project could be extended to evaluating different implementations of priority queues as well.

\subsection{Set Operations}
A minimal set data structure should be able to determine whether or not a given element is a member of the set. A dynamic set differs from a static set in that it supports operations to add and/or remove elements from the set dynamically. In our experiments, we will start out with an empty set and only allow for insertion of new elements. This means the our dynamic set must support two basic operations:
\begin{itemize}
\item \verb#insert(S,X)# -- Inserts $X$ into set $S$, if it is not yet present.
\item \verb#contains(S,X)# -- Returns whether set $S$ contains $X$.
\end{itemize}
In practice, it is useful to be able to test if a set contains an element and add this element if it does not. This can be implemented by first testing for the existence, and then conditionally inserting the element (as is done in program \ref{prog-search}) but many data structures allow to combine this into a single operation. To take advantage of this possibility, we will require that the \verb#insert# operation also returns whether or not the element was newly inserted.

\subsection{Set Implementations}
Many different data structures have been developed that can be used to efficiently implement the operations described above. For each of the categories identified earlier, at least one data structure will be implemented:
\begin{itemize}
\item \textbf{Cache-unaware}: hash tables. Hash tables are widely used in practice and noted for good performance if the data set fits in main memory, although they have also been used as an index structure for data stored on disk. Other possibilities are Patricia trees \cite{morrison1968ppa}, Splay trees \cite{sleator1985sab} or one of the many other variations on search trees.
\item \textbf{Cache-aware}: B-trees. The B-tree is the standard choice for storing ordered data on disk, and depends on a page size being selected that corresponds with the size of data blocks that can be efficiently transferred.
\item \textbf{Cache-oblivious}: the B-tree-like data structure proposed by Bender, Duan, Iacono and Wu. Since it provides functionality comparable to that of a normal B-tree, this seems like a fair candidate for a comparison. An alternative is the cache-oblivious exponential search tree proposed by Rahman, Cole and Raman. \cite{rahman2001opd}.
% The second, if I can find it! (If so, add description to related work)
\end{itemize}

The cache-oblivious data structures have been developed recently and are not commonly used in practice. These will have to be implemented from scratch, using existing publications as a guideline. The traditional data structures are commonly used and therefore have high-quality implementations already available as software libraries. For this research, however, it is undesirable to use existing libraries, for two reasons. First, many existing implementations support additional operations (such as removal of elements, locking and synchronization, atomic transactions, et cetera) which are not used in our experiments, but which may harm the performance of these implementations. Second, many established libraries have been thoroughly optimized while our newly implemented data structures have not. This may give the existing libraries an unfair advantage.

In an attempt to reduce the bias caused by differences in functionality and quality between existing and newly developed libraries, all data structures used in the performance evaluation will be implemented from scratch.

\subsection{Measurement}
% TODO: which hardware and software platform used, et cetera. Do we discuss this?
As described in the problem statement, the main metric of interest is the total execution time of our algorithm. To be able to quantify the obtained results, we also need to be able to estimate the overhead incurred by the use of our testing framework and other outside influences. Two methods will be employed:
\begin{itemize}
\item Profiling tools such as OProfile \cite{oprofile} can be used to determine which part of the execution time was spent executing the testing framework, with a small overhead during execution.
\item A mock set implementation will be written that does not implement set functionality itself, but reads the answers to queries from a file. This can be used to give an upper bound on the overhead caused by the testing framework.
\end{itemize}

As a secondary metric, we are interested in the memory use of the different data structures. This is harder to quantify, but the following methods may be employed:
\begin{itemize}
\item The maximum resident set size of a process is a lower bound on the amount of memory a process used.
\item The number of times a memory page has to be read from or written to disk is an indication of how often the application exceeds the available main memory.
\end{itemize}
Note that these metrics only given an indication of memory use on the boundary between main memory and disk storage.

\subsection{Test Data}
The test data will be generated with state space generation software that generates synthetic test data based on parameters supplied by the user. This allows us to assess the effect of varying parameters on the performance of the data structures being evaluated. Since the same software is already used in state search related research, the generated test cases are expected to be representative of realistic data sets.

The results may depend on the properties of the generated test data. At least three variables can be identified:
\begin{itemize}
\item The size of the data set: Cache-oblivious data structures usually rely on asymptotic optimality, which means that they may not perform well on small data sets. It would be interesting to see what the effect of various data sizes is on their relative performance.
\item The ratio between queries and insertions: for a particular data structure, it may be much more efficient to query the existence of a value in the set than to add a new value to it. We should either ensure our ratio between queries and insertions is realistic, or run experiments with varying ratios to see how this affects the performance of different data structures. In the case of state search algorithms, a sparse state space (few transitions between states) results in a high number of insertions.
\item Data access patterns: some data structures perform better when accessing elements that have recently been accessed before. For example, splay trees explicitly support this principle. Again, we should either ensure our access patterns are realistic, or test with varying access patterns and see how data structures are affected.
\end{itemize}

\section{Planning}
See table \ref{tab-planning}.

\begin{table*}
\begin{center}
\begin{tabular}{ | l | l | l | }
\hline
\textbf{Task} & \textbf{Result} & \textbf{Deadline} \\
\hline
Finish research proposal
    & Final research proposal
    & March 27th \\
Study literature
    & "Related Work" section
    & April 4th \\
Implement algorithms
    & Working code
    & April 28th \\
Finish paper contents
    & Complete paper for peer review
    & May 6th \\
Revise paper
    & Revised paper
    & May 23rd \\
Final paper formatting
    & Finished paper
    & June 11th \\
Prepare presentation
    & Presentation
    & June 20th \\
\hline
\end{tabular}
\caption{Planning}
\label{tab-planning}
\end{center}
\end{table*}

\bibliographystyle{plain}
\bibliography{proposal}
\end{document}
